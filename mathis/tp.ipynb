{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cc8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import httplib2\n",
    "import urllib\n",
    "from urllib.parse import urlencode, quote_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36372add",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Crawling the data\n",
    "\n",
    "**Q2.1** Which Wikipedia category is crawled in this script?\n",
    "\n",
    "**Q2.2** What does this script output?\n",
    "\n",
    "**Q2.3** When running the script `crawl.py`, what should the file `wiki.lst` contain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc393db",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "**R2.1:** In `crawl.py`, the Wikipedia category crawled is **Biology**, as shown in the following code snippet:\n",
    "```python\n",
    "categoryToCrawl = \"Category:Biology\"\n",
    "pagesToDw = getPages(categoryToCrawl)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryToCrawl = \"Category:Biology\"\n",
    "crawlingDepth = 2\n",
    "\n",
    "def getPages(category):\n",
    "\th = httplib2.Http()\n",
    "\tparams = dict()\n",
    "\tparams[\"cmlimit\"] = \"500\"\n",
    "\tparams[\"list\"] = \"categorymembers\"\n",
    "\tparams[\"action\"] = \"query\"\n",
    "\tparams[\"format\"] = \"json\"\n",
    "\tparams[\"cmtitle\"] = category\n",
    "\tencodedParams = urlencode(params)\n",
    "\t(resp_headers, content) = h.request(\"http://en.wikipedia.org/w/api.php?\" + encodedParams, \"GET\")\n",
    "\tjsonContent = content.decode('utf-8')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tj = json.loads(jsonContent)[\"query\"][\"categorymembers\"]\n",
    "\texcept json.JSONDecodeError: # Added exception handling\n",
    "\t\treturn []\n",
    "\n",
    "\treturn j\n",
    "\n",
    "pagesToDw = getPages(categoryToCrawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aab28",
   "metadata": {},
   "source": [
    "**R2.2:** The output of `crawl.py` is the depth of the current crawling as well as the number of pages to download as shown in the following code snippet:\n",
    "```python\n",
    "print(\"Crawling at depth\",depth,\". Pages to dw:\",len(pagesToDw))\n",
    "```\n",
    "It also creates a file `wiki.lst` for which we discuss the content in the next question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0684fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling at depth 0 . Pages to dw: 63\n",
      "Crawling at depth 1 . Pages to dw: 1474\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.lst\",'w',encoding='utf-8') as outFile:\n",
    "\tfor depth in range(crawlingDepth):\n",
    "\t\tprint(\"Crawling at depth\",depth,\". Pages to dw:\",len(pagesToDw))\n",
    "\t\tdeeperLevelPages = list()\n",
    "\t\tfor page in pagesToDw:\n",
    "\t\t\tpageTitle = page[\"title\"]\n",
    "\t\t\tif pageTitle.startswith(\"Category:\"):\n",
    "\t\t\t\tdeeperLevelPages += getPages(pageTitle)\n",
    "\t\t\toutFile.write(pageTitle+\"\\n\")\n",
    "\t\tpagesToDw = deeperLevelPages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ad4bf",
   "metadata": {},
   "source": [
    "**R2.3:** he file `wiki.lst` should contain the list of titles of the pages visited by the crawler, as illustrated in the following code snippet:\n",
    "```python\n",
    "outFile.write(pageTitle+\"\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06c940",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Downloading the data\n",
    "\n",
    "**Q3.1** How many pages per batch is downloaded ?\n",
    "\n",
    "**Q3.2** What API of wikipedia is used to download a set of pages ?\n",
    "\n",
    "**Q3.3** How does the crawling work here ? \n",
    "\n",
    "**Q3.4** By going to the API page in your browser, and reading the documentation paragraph, can you tell in what format the pages will be encoded ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6ca4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Parsing the data\n",
    "\n",
    "**Q4.1** From the code, how are encoded the two matrices (i.e what type of Python object) ? What is the name of this encoding ?\n",
    "\n",
    "**Q4.2** Take a look at the database of Wikipedia documents in the `dws` folder, for example using the command `vi` or `less`. How are the links encoded in the wiki language ? \n",
    "\n",
    "**Q4.3** Complete regular expresson for extracting the links.\n",
    "\n",
    "**Q4.4** Find and complete a simple regular expression for removing noisy data such as external links (outside of Wikipedia) and info boxes.\n",
    "\n",
    "**Q4.5** Implement your regular expression in Python such that the first group contains everything in the link (the target as well as its potential displayed text).\n",
    "\n",
    "**Q4.6** The current implementation builds a doc-tok matrix. You need to transpose it to have a reverse sparse index. As this looks a bit underoptimal, try also to build directly the reverse version when parsing the documents (i.e create directly a tok-doc index) and measure the performance (in per cent of execution time) you gain/loose? How do you explain that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcae6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Page Rank of the Document\n",
    "\n",
    "**Q5.1** In the random surfer model, at each iteration, random clicks are \"simulated\" with a given probability. Complete the code with the correct probability.\n",
    "\n",
    "**Q5.2** What is the name of the effect we circumvent by adding `sourceVector` to the newly computed page rank vector pageRanksNew ? \n",
    "\n",
    "**Q5.3** Implement the formula of the convergence $\\delta$.\n",
    "\n",
    "**Q5.4** Run the PageRank program in interactive mode `python3 -i pageRank.py`, and use the Python interface to answer the following :\n",
    "- How many iteration did it need to converge ? \n",
    "- What is the page rank of \"DNA\" ? \n",
    "- What is the page with the highest rank ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec291748",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Woogle!\n",
    "\n",
    "**Q6.1** What type of page is selected by the vector model ? By looking at the Wikipedia page, how can you explain it ? What is the ,ame of this classical cheating ? \n",
    "\n",
    "**Q6.2** Propose and implement a way of correcting this phenomenon. Check if this correct the effect for the top 15 pages.\n",
    "\n",
    "**Q6.3** Take a look at vector model rankings for your query. What is the rank of the page \"Bacterial Evolution\" ? Is it expected ? How would you correct for it (see extra section) ? Play arounf with standard queries and try to understand the behavior. You can try the following queries (\"dna\", \"darwin\", \"crispr\") while varying\n",
    "- ranking and not ranking to check the difference\n",
    "- correcting or not for \"classical cheating\"\n",
    "- varying the number of results to be ranked (say 2, 10, 20, 200)\n",
    "\n",
    "**Q6.4** What is your feeling about the right parameters ? Is it better to rank with pageRank or not ? \n",
    "\n",
    "**Q6.5** Can you devise a way of automating the search for the right parameters ? If you think that you have somehow limited data, try to get more data and see if it solves your problem.\n",
    "\n",
    "**Q6.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a2657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Extras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
