{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cc8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import httplib2\n",
    "import urllib\n",
    "from urllib.parse import urlencode, quote_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36372add",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Crawling the data\n",
    "\n",
    "**Q2.1** Which Wikipedia category is crawled in this script?\n",
    "\n",
    "**Q2.2** What does this script output?\n",
    "\n",
    "**Q2.3** When running the script `crawl.py`, what should the file `wiki.lst` contain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc393db",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "**R2.1:** In `crawl.py`, the Wikipedia category crawled is **Biology**, as shown in the following code snippet:\n",
    "```python\n",
    "categoryToCrawl = \"Category:Biology\"\n",
    "pagesToDw = getPages(categoryToCrawl)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryToCrawl = \"Category:Biology\"\n",
    "crawlingDepth = 2\n",
    "\n",
    "def getPages(category):\n",
    "\th = httplib2.Http()\n",
    "\tparams = dict()\n",
    "\tparams[\"cmlimit\"] = \"500\"\n",
    "\tparams[\"list\"] = \"categorymembers\"\n",
    "\tparams[\"action\"] = \"query\"\n",
    "\tparams[\"format\"] = \"json\"\n",
    "\tparams[\"cmtitle\"] = category\n",
    "\tencodedParams = urlencode(params)\n",
    "\t(resp_headers, content) = h.request(\"http://en.wikipedia.org/w/api.php?\" + encodedParams, \"GET\")\n",
    "\tjsonContent = content.decode('utf-8')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tj = json.loads(jsonContent)[\"query\"][\"categorymembers\"]\n",
    "\texcept json.JSONDecodeError: # Added exception handling\n",
    "\t\treturn []\n",
    "\n",
    "\treturn j\n",
    "\n",
    "pagesToDw = getPages(categoryToCrawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aab28",
   "metadata": {},
   "source": [
    "**R2.2:** The output of `crawl.py` is the depth of the current crawling as well as the number of pages to download as shown in the following code snippet:\n",
    "```python\n",
    "print(\"Crawling at depth\",depth,\". Pages to dw:\",len(pagesToDw))\n",
    "```\n",
    "It also creates a file `wiki.lst` for which we discuss the content in the next question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0684fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling at depth 0 . Pages to dw: 63\n",
      "Crawling at depth 1 . Pages to dw: 1474\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.lst\",'w',encoding='utf-8') as outFile:\n",
    "\tfor depth in range(crawlingDepth):\n",
    "\t\tprint(\"Crawling at depth\",depth,\". Pages to dw:\",len(pagesToDw))\n",
    "\t\tdeeperLevelPages = list()\n",
    "\t\tfor page in pagesToDw:\n",
    "\t\t\tpageTitle = page[\"title\"]\n",
    "\t\t\tif pageTitle.startswith(\"Category:\"):\n",
    "\t\t\t\tdeeperLevelPages += getPages(pageTitle)\n",
    "\t\t\toutFile.write(pageTitle+\"\\n\")\n",
    "\t\tpagesToDw = deeperLevelPages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ad4bf",
   "metadata": {},
   "source": [
    "**R2.3:** he file `wiki.lst` should contain the list of titles of the pages visited by the crawler, as illustrated in the following code snippet:\n",
    "```python\n",
    "outFile.write(pageTitle+\"\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06c940",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Downloading the data\n",
    "\n",
    "**Q3.1** How many pages per batch is downloaded ?\n",
    "\n",
    "**Q3.2** What API of wikipedia is used to download a set of pages ?\n",
    "\n",
    "**Q3.3** How does the crawling work here ? \n",
    "\n",
    "**Q3.4** By going to the API page in your browser, and reading the documentation paragraph, can you tell in what format the pages will be encoded ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef3adf",
   "metadata": {},
   "source": [
    "**R3.1:** We split the pages into batches of 3000 pages, as we are running linux\n",
    "```bash\n",
    "sort -u $1|grep \"[A-Za-z0-9]\" > $1.uniq\n",
    "split $1.uniq -l 3000 --additional-suffix=.chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d852f",
   "metadata": {},
   "source": [
    "**R3.2:** The API from Wikipedia used in this script is \"https://en.wikipedia.org/wiki/Special:Export\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501baf4",
   "metadata": {},
   "source": [
    "**R3.3:** The crawler takes the list of page titles stored in `wiki.lst`, splits it into batches of 3000 pages, and generates URLs from each batch. These batches are then submitted to Wikipediaâ€™s MediaWiki interface to download the corresponding pages in XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d143dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7589k   0  7550k 100 40401  1131k  6055   0:00:06  0:00:06 --:--:--  1438k\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# --- 0. Arguments to run in jupyter cell ---\n",
    "INPUT_FILE=\"wiki.lst\" \n",
    "\n",
    "# --- 1. Clean page titles : sort and remove duplicates ---\n",
    "sort -u $INPUT_FILE|grep \"[A-Za-z0-9]\" > $INPUT_FILE.uniq\n",
    "\n",
    "# --- 2. Split into batches ---\n",
    "split $INPUT_FILE.uniq -l 3000 --additional-suffix=.chunks\n",
    "\n",
    "# --- 3. Create Output Directory ---\n",
    "mkdir -p dws\n",
    "\n",
    "# --- 4. Encode in the right format for url ---\n",
    "alias urlencode='python3 -c \"import sys, urllib.parse as ul; \\\n",
    " f=open(sys.argv[1],encoding=\\\"utf-8\\\"); \\\n",
    " print(\\\"\\\".join([ul.quote_plus(l) for l in f ]))\"'\n",
    "\n",
    "# --- 5. Actually download ---\n",
    "for f in `ls *.chunks`; do\n",
    "\tcat $f| awk  '{if(page!=\"\") {page = page\"\\r\\n\"} page = page$0}END{print(page)}' > $f.post\n",
    "\tpages=$(urlencode $f.post)\n",
    "\tcurl -X POST -b \"WMF-Last-Access-Global=29-Oct-2018&WMF-Last-Access=29-Oct-2018&GeoIP=FR%3AARA%3AGrenoble%3A45.17%3A5.72%3Av4&VEE=wikitext\" -H \"Host: en.wikipedia.org\" -H \"User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0\" -H \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\" -H \"Accept-Language: en-US,en;q=0.5\" -H \"Accept-Encoding: gzip, deflate, br\" -H \"Referer: https://en.wikipedia.org/wiki/Special:Export\" -H \"Content-Type: application/x-www-form-urlencoded\"  -H \"Cookie: WMF-Last-Access-Global=29-Oct-2018; WMF-Last-Access=29-Oct-2018; GeoIP=FR:ARA:Grenoble:45.17:5.72:v4; VEE=wikitext\" -H \"Connection: keep-alive\" -H \"Upgrade-Insecure-Requests: 1\" -d \"catname=&curonly=1&wpDownload=1&wpEditToken=%2B%5C&title=Special%3AExport&pages=$pages\" https://en.wikipedia.org/wiki/Special:Export > dws/$f.gz;\n",
    "\trm -f $f.post;\n",
    "\tgzip -d dws/$f.gz;\n",
    "done;\n",
    "\n",
    "rm -f *.chunks $INPUT_FILE.uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeebfbb",
   "metadata": {},
   "source": [
    "**R3.4:** The documentation tells us that the format used in XML as \"You can export the text and editing history of a particular page or set of pages wrapped in XML.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6ca4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Parsing the data\n",
    "\n",
    "**Q4.1** From the code, how are encoded the two matrices (i.e what type of Python object) ? What is the name of this encoding ?\n",
    "\n",
    "**Q4.2** Take a look at the database of Wikipedia documents in the `dws` folder, for example using the command `vi` or `less`. How are the links encoded in the wiki language ? \n",
    "\n",
    "**Q4.3** Complete regular expresson for extracting the links.\n",
    "\n",
    "**Q4.4** Find and complete a simple regular expression for removing noisy data such as external links (outside of Wikipedia) and info boxes.\n",
    "\n",
    "**Q4.5** Implement your regular expression in Python such that the first group contains everything in the link (the target as well as its potential displayed text).\n",
    "\n",
    "**Q4.6** The current implementation builds a doc-tok matrix. You need to transpose it to have a reverse sparse index. As this looks a bit underoptimal, try also to build directly the reverse version when parsing the documents (i.e create directly a tok-doc index) and measure the performance (in per cent of execution time) you gain/loose? How do you explain that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcae6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Page Rank of the Document\n",
    "\n",
    "**Q5.1** In the random surfer model, at each iteration, random clicks are \"simulated\" with a given probability. Complete the code with the correct probability.\n",
    "\n",
    "**Q5.2** What is the name of the effect we circumvent by adding `sourceVector` to the newly computed page rank vector pageRanksNew ? \n",
    "\n",
    "**Q5.3** Implement the formula of the convergence $\\delta$.\n",
    "\n",
    "**Q5.4** Run the PageRank program in interactive mode `python3 -i pageRank.py`, and use the Python interface to answer the following :\n",
    "- How many iteration did it need to converge ? \n",
    "- What is the page rank of \"DNA\" ? \n",
    "- What is the page with the highest rank ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec291748",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Woogle!\n",
    "\n",
    "**Q6.1** What type of page is selected by the vector model ? By looking at the Wikipedia page, how can you explain it ? What is the ,ame of this classical cheating ? \n",
    "\n",
    "**Q6.2** Propose and implement a way of correcting this phenomenon. Check if this correct the effect for the top 15 pages.\n",
    "\n",
    "**Q6.3** Take a look at vector model rankings for your query. What is the rank of the page \"Bacterial Evolution\" ? Is it expected ? How would you correct for it (see extra section) ? Play arounf with standard queries and try to understand the behavior. You can try the following queries (\"dna\", \"darwin\", \"crispr\") while varying\n",
    "- ranking and not ranking to check the difference\n",
    "- correcting or not for \"classical cheating\"\n",
    "- varying the number of results to be ranked (say 2, 10, 20, 200)\n",
    "\n",
    "**Q6.4** What is your feeling about the right parameters ? Is it better to rank with pageRank or not ? \n",
    "\n",
    "**Q6.5** Can you devise a way of automating the search for the right parameters ? If you think that you have somehow limited data, try to get more data and see if it solves your problem.\n",
    "\n",
    "**Q6.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a2657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Extras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
